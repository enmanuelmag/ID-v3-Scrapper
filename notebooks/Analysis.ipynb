{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "#%reload_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "sys.path.append(\"../model\")\n",
    "\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import mlflow as mlf\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "from pysentimiento.preprocessing import preprocess_tweet\n",
    "from analyzer_blstm import (\n",
    "    AnalyzerForSequenceClassification,\n",
    "    create_analyzer_blstm,\n",
    ")\n",
    "\n",
    "logging.getLogger(\"Utils\").setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "ROBERTUITO_RUN_ID = \"52089d0757e64bf588f2c75e439ae4e0\"  # FINAL\n",
    "\n",
    "ROBERTUITO_BLSTM_RUN_ID = \"631eeac0d2d84a24a6b41a7ff6cc3ba4\"  # FINAL\n",
    "\n",
    "MIN_TEXT_LENGTH = 4\n",
    "\n",
    "PIVOT_DATE = \"2025-01-10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.match(r\"\\d{4}[-/]\\d{1,2}[-/]\\d{1,2}\", \"2023-11-23\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_date_tiktok(text: str, pivot_date=PIVOT_DATE) -> pd.Timestamp:\n",
    "    text = str(text).strip()\n",
    "\n",
    "    def _parse_date(_text: str) -> pd.Timestamp:\n",
    "        formats = [\"%d-%m-%Y\", \"%d/%m/%Y\", \"%Y-%m-%d\", \"%Y/%m/%d\"]\n",
    "        for fmt in formats:\n",
    "            try:\n",
    "                return pd.to_datetime(_text, format=fmt, errors=\"raise\")\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    if text and re.match(r\"\\d{4}[-/]\\d{1,2}[-/]\\d{1,2}\", text):\n",
    "        return _parse_date(text)\n",
    "    elif text and re.match(r\"\\d{1,2}[-/]\\d{1,2}[-/]\\d{4}\", text):\n",
    "        return _parse_date(text)\n",
    "    elif text and re.match(r\"\\d{1,2}[-/]\\d{1,2}\", text):\n",
    "        if \"-\" in text:\n",
    "            _, month = text.split(\"-\")\n",
    "        else:\n",
    "            _, month = text.split(\"/\")\n",
    "        if month == 1:\n",
    "            text = f\"{text}-2025\"\n",
    "        else:\n",
    "            text = f\"{text}-2024\"\n",
    "        text = text.replace(\"/\", \"-\")\n",
    "        return pd.to_datetime(text, format=\"%m-%d-%Y\", errors=\"raise\")\n",
    "    elif \"ago\" in text:\n",
    "        pivot_date = pd.to_datetime(pivot_date, errors=\"raise\", format=\"%Y-%m-%d\")\n",
    "        if \"d\" in text:\n",
    "            date = pivot_date - pd.Timedelta(days=int(text.split(\"d ago\")[0]))\n",
    "        elif \"h\" in text:\n",
    "            date = pivot_date - pd.Timedelta(hours=int(text.split(\"h ago\")[0]))\n",
    "        elif \"w\" in text:\n",
    "            date = pivot_date - pd.Timedelta(weeks=int(text.split(\"w ago\")[0]))\n",
    "        elif \"m\" in text:\n",
    "            date = pivot_date - pd.Timedelta(month=int(text.split(\"m ago\")[0]))\n",
    "        else:\n",
    "            date = pivot_date\n",
    "        return date\n",
    "\n",
    "    return pd.NaT\n",
    "\n",
    "\n",
    "def str_to_date_twitter(text: str) -> pd.Timestamp:\n",
    "    text = str(text).strip()\n",
    "\n",
    "    def _parse_timestamp(serial: int) -> pd.Timestamp:\n",
    "        # Fecha base en Excel (1 de enero de 1900)\n",
    "        fecha_base = pd.Timestamp(\"1900-01-01\")\n",
    "\n",
    "        # Convertir el n√∫mero de serie a una fecha\n",
    "        fecha = fecha_base + pd.Timedelta(days=serial - 2)\n",
    "\n",
    "        # Formatear la fecha en el formato \"dd/mm/yyyy\"\n",
    "        fecha_formateada = fecha.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "        return fecha_formateada\n",
    "\n",
    "    if \"Z\" in text:\n",
    "        return pd.to_datetime(text, format=\"%Y-%m-%dT%H:%M:%S.%fZ\", errors=\"raise\")\n",
    "    elif re.match(r\"\\d{4,6}\", text):\n",
    "        return _parse_timestamp(int(text))\n",
    "\n",
    "    return pd.NaT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_path = os.path.abspath(os.path.join(\"..\", \"model\", \"mlruns\"))\n",
    "mlf.set_tracking_uri(\"file:/ \" + runs_path)\n",
    "\n",
    "columns = [\n",
    "    \"run_id\",\n",
    "    \"status\",\n",
    "    \"params.lstm_hidden_dim\",\n",
    "    \"params.lstm_num_layers\",\n",
    "    \"metrics.train_runtime\",\n",
    "    \"metrics.eval_macro_f1\",\n",
    "    \"metrics.train_loss\",\n",
    "    \"metrics.eval_macro_recall\",\n",
    "    \"metrics.eval_macro_precision\",\n",
    "    \"artifact_uri\",\n",
    "]\n",
    "\n",
    "runs = mlf.search_runs(\n",
    "    filter_string='status=\"FINISHED\"', order_by=[\"metrics.eval_macro_f1 DESC\"]\n",
    ")[columns]\n",
    "\n",
    "# search\n",
    "run_torch = runs[runs[\"run_id\"] == ROBERTUITO_RUN_ID].iloc[0]\n",
    "roubertuito = f'{run_torch[\"artifact_uri\"]}/model'.replace(\"file:///\", \"\")\n",
    "roubertuito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_analyzer = create_analyzer_blstm(model_path=roubertuito)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_analysis_x(\n",
    "    row: pd.Series, model_analyzer: AnalyzerForSequenceClassification\n",
    "):\n",
    "\n",
    "    text = row[\"text\"]\n",
    "\n",
    "    if not text or len(text) < MIN_TEXT_LENGTH:\n",
    "        return {\n",
    "            \"text\": text,\n",
    "            \"analysis\": None,\n",
    "        }\n",
    "\n",
    "    text_analysis = model_analyzer.predict(text).probas\n",
    "\n",
    "    max_proba = max(text_analysis.items(), key=lambda x: x[1])[0]\n",
    "\n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"analysis\": {\n",
    "            \"max_proba\": max_proba,\n",
    "            \"probas\": text_analysis,\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = pd.read_excel(\"../data/append_tweets.xlsx\")\n",
    "df_x[\"text\"] = df_x[\"text\"].astype(str)\n",
    "df_x[\"date_parsed\"] = df_x[\"timestamp\"].progress_apply(lambda x: str_to_date_twitter(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x[df_x[\"date_parsed\"].isna()][[\"timestamp\", \"date_parsed\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f9cdc90730d41d9a559610357bfc904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1646 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "analysis = df_x.progress_apply(\n",
    "    lambda row: process_analysis_x(row, model_analyzer), axis=1\n",
    ")\n",
    "\n",
    "df_x[\"analysis\"] = analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x.to_excel(\"../data/append_tweets_analysis_v2.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis tiktoks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_analysis_tiktok(\n",
    "    row: pd.Series, model_analyzer: AnalyzerForSequenceClassification\n",
    "):\n",
    "\n",
    "    description_text = row[\"description\"]\n",
    "\n",
    "    comments_text = row[\"comments_text\"].split(\"|\")\n",
    "\n",
    "    comments_text = [\n",
    "        comment for comment in comments_text if len(comment) >= MIN_TEXT_LENGTH\n",
    "    ]\n",
    "\n",
    "    description_analysis = None\n",
    "\n",
    "    if not description_text or len(description_text) < MIN_TEXT_LENGTH:\n",
    "        probas_desc = model_analyzer.predict(description_text).probas\n",
    "\n",
    "        if len(probas_desc) == 0:\n",
    "            probas_desc = None\n",
    "        else:\n",
    "            max_proba_description = max(probas_desc.items(), key=lambda x: x[1])[0]\n",
    "            description_analysis = {\n",
    "                \"sentiment\": max_proba_description,\n",
    "                \"probas\": probas_desc,\n",
    "            }\n",
    "    else:\n",
    "        description_analysis = None\n",
    "\n",
    "    comments_analysis = []\n",
    "\n",
    "    for comment_text in tqdm(\n",
    "        comments_text, desc=\"Analyzing comments\", leave=False, unit=\"comment\"\n",
    "    ):\n",
    "        if len(comment_text) < MIN_TEXT_LENGTH:\n",
    "            comments_analysis.append({\"text\": comment_text, \"analysis\": None})\n",
    "\n",
    "        probas = model_analyzer.predict(comment_text).probas\n",
    "\n",
    "        if len(probas) == 0:\n",
    "            comments_analysis.append(\n",
    "                {\n",
    "                    \"text\": comment_text,\n",
    "                    \"analysis\": None,\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            max_proba = max(probas.items(), key=lambda x: x[1])[0]\n",
    "\n",
    "            comments_analysis.append(\n",
    "                {\n",
    "                    \"text\": comment_text,\n",
    "                    \"analysis\": {\n",
    "                        \"sentiment\": max_proba,\n",
    "                        \"probas\": probas if len(probas) > 0 else None,\n",
    "                    },\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return {\n",
    "        \"description\": {\n",
    "            \"text\": description_text,\n",
    "            \"analysis\": description_analysis,\n",
    "        },\n",
    "        \"comments\": comments_analysis,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tiktok = pd.read_excel(\"../data/append_tik_tok.xlsx\")\n",
    "df_tiktok[\"description\"] = df_tiktok[\"description\"].astype(str)\n",
    "df_tiktok[\"comments_text\"] = df_tiktok[\"comments_text\"].astype(str)\n",
    "df_tiktok[\"date_parsed\"] = df_tiktok[\"date\"].progress_apply(\n",
    "    lambda x: str_to_date_tiktok(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = df_tiktok.progress_apply(\n",
    "    lambda row: process_analysis_tiktok(row, model_analyzer), axis=1\n",
    ")\n",
    "\n",
    "df_tiktok[\"analysis\"] = analysis\n",
    "df_tiktok.to_excel(\"../data/append_tik_tok_analysis_v2.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
